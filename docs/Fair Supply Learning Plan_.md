

# **A Strategic Learning Plan for ESG Technology at Fair Supply**

## **Part 1: Deconstructing Fair Supply: The Anatomy of an ESG Intelligence Platform**

A deep understanding of a target company's mission, technology, and market context is the essential foundation upon which a successful learning plan is built. For a specialized, purpose-driven organization like Fair Supply, this understanding must go beyond a surface-level reading of its website. It requires a strategic deconstruction of its core value proposition, its technological moat, and the complex problem space it navigates. Aligning one's skills with this deep context transforms a candidate from merely qualified to ideally suited.

### **1.1. Mission, Vision, and Core Value Proposition**

Fair Supply is fundamentally a purpose-driven Environmental, Social, and Governance (ESG) technology and data business.1 Its mission is to enable ethical investment and purchasing decisions through "unrivalled global supply chain transparency and intelligence".1 The company's origin story is a critical key to understanding its DNA: it was born from the collaboration between a human rights lawyer and an industrial mathematician.1 This fusion of legal-ethical rigor and mathematical precision is not merely a marketing narrative; it is the company's core differentiator in a crowded ESG market.

Unlike many platforms that rely on opaque scoring or broad industry averages, Fair Supply emphasizes providing "transparent, supplier-specific results" and "audit-ready, defensible results".3 This commitment stems from its founding principle of combining deep subject matter expertise with precise data science.1 The platform is designed to move clients from "blind spots to action" across critical risk domains, unifying supplier assessment, engagement, and compliance reporting into a single, auditable workflow.3 For a prospective technical candidate, this signals that the required standard is not just functional code, but methodologies that are transparent, explainable, and can withstand the scrutiny of auditors and regulators. The work must reflect an ethos of precision and defensibility, mirroring the company's own market positioning.

### **1.2. The Technological Core: The "Integrated Assessment Engine" and Tier 10 Visibility**

At the heart of Fair Supply's platform lies its proprietary "Integrated Assessment Engine".4 This engine is the technical manifestation of its mission, designed to make the "invisible visible" by mapping ESG risk through an unprecedented

**10 tiers** of a client's supply chain.3 The core methodology underpinning this capability is the

**Multi-Region Input-Output (MRIO) model**.3 An MRIO model is a powerful economic analysis tool that represents the entire global economy as a vast matrix of inter-industry financial flows. It quantifies how the output of one industrial sector becomes the input for another, across different countries.

The application of this methodology to supply chain analysis reveals the company's fundamentally graph-centric worldview. A supply chain is, by its nature, a graph, where companies, facilities, and products are the **nodes**, and transactions, shipments, and ownership stakes are the **edges**.5 When Fair Supply ingests a client's basic expense data—a list of Tier 1 suppliers and the annual spend per supplier—it is effectively seeding this massive global economic graph with the client's specific starting points.3 The "Integrated Assessment Engine" then traces these financial flows backward through the MRIO graph, from the Tier 1 supplier to its suppliers (Tier 2), and their suppliers (Tier 3), and so on, up to Tier 10\.3

This process of tracing risk through ten tiers is, algorithmically, a multi-hop graph traversal problem. It implies that Fair Supply's core data structure is not a simple relational database but a massive knowledge graph representing the intricate web of global commerce. Consequently, technologies like Graph Neural Networks (GNNs) and graph databases are not merely adjacent or "nice-to-have" skills; they are central to the company's unique technological capability. A candidate's learning path must therefore be centered on the principles and practices of large-scale graph data modeling and analysis, as this is the technical foundation of Fair Supply's "Tier 10 Visibility".3

### **1.3. Product Landscape: Modern Slavery, Emissions, and Biodiversity**

Fair Supply has strategically focused its powerful engine on three of the most complex and high-stakes ESG risk domains: modern slavery, greenhouse gas (GHG) emissions, and biodiversity.3 This specialization distinguishes it from generic ESG platforms that may offer broad, but shallow, coverage.

* **Modern Slavery:** The company's journey began with the Australian Modern Slavery Act of 2018, and its first product was designed to footprint modern slavery risk in supply chains and investment portfolios.1 The platform enables organizations to identify, address, and monitor these risks to comply with evolving global legislation.4  
* **Emissions:** Fair Supply provides a comprehensive carbon accounting solution, measuring not only direct operational emissions (Scope 1\) and purchased energy (Scope 2), but also the far more complex upstream and downstream supply chain emissions (Scope 3).1 This capability is critical for clients needing to comply with frameworks like the TCFD, ISSB, and CSRD.7 The company has partnered with major players like the London Stock Exchange Group to automate the measurement of these emissions.1  
* **Biodiversity:** In a planet-first initiative, Fair Supply launched an extinction-risk footprinting product in collaboration with the IBAT Alliance.1 The platform quantifies extinction risk within supply chains, identifying the specific locations and species most impacted by a company's industrial activities and suggesting potential mitigation strategies.4

This product-level focus provides a clear directive for an aspiring candidate. Instead of pursuing a generic "ESG risk" project, a more impactful approach is to develop projects that mirror this specialization. Creating a portfolio piece that specifically targets one of these three domains—using relevant, domain-specific data and methodologies—demonstrates direct alignment with Fair Supply's product roadmap and an understanding of their clients' most pressing needs.

### **1.4. The Regulatory and Competitive Landscape**

The rapid growth and market opportunity for Fair Supply are directly fueled by a tightening web of global ESG regulations. These frameworks are transforming ESG from a voluntary corporate social responsibility activity into a mandatory, auditable reporting requirement. The platform is explicitly designed to be "Built for Procurement, Ready for ESG," generating reports aligned with major global laws and standards.3 A candidate who understands these regulatory drivers is infinitely more valuable, as they comprehend the fundamental business problem Fair Supply is solving.

Key frameworks shaping this landscape include:

* **TCFD (Task Force on Climate-Related Financial Disclosures):** Though now subsumed by the ISSB, the TCFD's four-pillar structure (Governance, Strategy, Risk Management, Metrics & Targets) has become the de facto blueprint for climate-related financial disclosure and is embedded in most subsequent regulations.9  
* **ISSB (International Sustainability Standards Board):** The ISSB is creating a global baseline for sustainability and climate reporting (IFRS S1 and S2) aimed at investors.10 Its standards are being adopted or aligned with by jurisdictions representing over half the global economy, signaling a move toward global convergence.9  
* **CSRD (Corporate Sustainability Reporting Directive):** This is the EU's flagship regulation, mandating extensive ESG disclosures under the European Sustainability Reporting Standards (ESRS). Crucially, it introduces the concept of **"double materiality,"** requiring companies to report not only on how ESG issues affect their business (financial materiality) but also on how their business impacts people and the environment (impact materiality).9 Its reach extends to many non-EU companies with significant operations in the EU.  
* **CSDDD (Corporate Sustainability Due Diligence Directive):** This EU directive goes beyond reporting to mandate that companies conduct due diligence to identify, prevent, and mitigate adverse human rights and environmental impacts within their own operations and across their value chains.11

This regulatory environment creates a significant demand for the kind of deep supply chain visibility that Fair Supply provides. In the competitive arena, Fair Supply is positioned against larger, more established players like EcoVadis.12 However, it differentiates itself by moving beyond ratings and focusing on providing objective, granular, and auditable data that allows companies to not just report on risk, but to re-architect their supply chains to eliminate it.2

A summary of these key frameworks highlights the compliance challenges that Fair Supply helps its clients navigate.

| Framework | Core Focus | Materiality Concept | Key Supply Chain Requirement |
| :---- | :---- | :---- | :---- |
| **TCFD** | Climate-related financial risks and opportunities | Financial Materiality | Disclosure of climate-related risks in the value chain |
| **ISSB** | Sustainability and climate-related information for investors | Financial Materiality | Disclosure of Scope 3 GHG emissions and climate risks/opportunities in the value chain |
| **CSRD** | Broad ESG reporting for multiple stakeholders | Double Materiality (Financial & Impact) | Reporting on value chain impacts, policies, and actions related to sustainability matters |
| **CSDDD** | Human rights and environmental due diligence | N/A (Focus on process) | Mandatory due diligence to identify, prevent, and mitigate adverse impacts in the value chain |

Data sourced from 9

## **Part 2: The Foundational Layer: Architecting an ESG Data Engineering Backbone**

Building a platform capable of delivering the "audit-ready, defensible results" promised by Fair Supply requires more than just clever algorithms; it demands an industrial-grade data engineering backbone.3 The process of ingesting, cleaning, integrating, and serving data must be meticulously designed for scalability, reliability, and, most importantly, traceability. The role of the data engineer is pivotal in this context, responsible for the collection, processing, and quality assurance that form the foundation of all subsequent ESG analysis and reporting.13

### **2.1. Principle 1: Designing for Auditability and Data Lineage**

For Fair Supply, where clients generate reports for regulators and auditors, data lineage is not a feature—it is a core requirement. Every data point, every risk assessment, must be traceable back to its origin. This principle of auditability must be woven into the fabric of the data architecture from the very beginning.

A robust learning path in this area involves mastering several key concepts and tools:

* **Data Modeling for Traceability:** The way data is structured is the first line of defense for auditability. This involves studying and implementing data modeling techniques that inherently track history. For instance, using **Slowly Changing Dimensions (SCDs)**, particularly Type 2, in a data warehouse allows the system to maintain a full historical record of supplier information. When a supplier's details change, a new record is added rather than overwriting the old one, ensuring that reports run on past data reflect the state of the world at that time.  
* **Data and Model Versioning:** Reproducibility is the cornerstone of defensibility. It must be possible to recreate any analysis or report exactly as it was first generated. This requires versioning not just code, but also the data and models used. Tools like **DVC (Data Version Control)** or Git LFS are essential for this purpose. They work alongside Git to track large data files and machine learning models, creating immutable snapshots of the entire analytical environment.  
* **Automated Data Quality Frameworks:** The integrity of the output is wholly dependent on the quality of the input. Manual data checks are not scalable or reliable. A candidate must learn to implement automated data quality frameworks. Libraries like **Great Expectations** or the testing features within **dbt** (data build tool) allow engineers to define data validation rules as code. These "expectations" (e.g., spend\_amount must be a positive number, supplier\_id must match a specific regex pattern) are run as part of the data pipeline, automatically profiling data, validating it, and alerting on any deviations. This ensures that corrupt or invalid data is caught and handled before it can contaminate downstream analyses.

### **2.2. Principle 2: Building Scalable and Orchestrated Data Pipelines**

Fair Supply analyzes trillions of dollars in spend and investment data, a scale that mandates highly scalable and automated data pipelines.1 Manually running scripts is untenable; workflows must be defined, scheduled, and monitored programmatically.

The industry standard for this task is **Apache Airflow**, an open-source platform for programmatically authoring, scheduling, and monitoring workflows. A candidate must achieve proficiency in Airflow:

* **Core Airflow Concepts:** The learning journey begins with understanding Airflow's core abstraction: the **Directed Acyclic Graph (DAG)**. A DAG is a Python script that defines a workflow as a collection of tasks and their dependencies. One must master the fundamental building blocks, including:  
  * **Operators:** These are the templates for individual tasks. Key operators to learn include the BashOperator (for running shell commands), the PythonOperator (for running Python functions), and, critically for data pipelines, SQL-aware operators like the SQLExecuteQueryOperator.14  
  * **Hooks and Connections:** Airflow needs to interact with external systems like databases and APIs. Hooks provide the interface to these systems, while Connections securely store the necessary credentials (e.g., database URIs, API keys) in the Airflow metadata database.14  
  * **TaskFlow API:** A more modern, Pythonic way to write DAGs, the TaskFlow API allows developers to turn Python functions directly into Airflow tasks using decorators (@task), simplifying the process of passing data between tasks.14  
* **Practical Implementation:** The best way to learn is by building. A crucial exercise is to follow a comprehensive tutorial to construct a realistic data pipeline.14 A canonical example involves creating a DAG that:  
  1. **Extracts** data from an external source, such as downloading a CSV file from a URL using a Python request.  
  2. **Loads** this raw data into a staging table in a relational database like PostgreSQL.  
  3. **Transforms** the data by running a series of SQL queries to clean, deduplicate, and standardize it.  
  4. Loads the cleaned data into a final production table.  
     This E-T-L pattern directly simulates the process of ingesting a client's supplier spend data for analysis.  
* **Containerization with Docker:** Modern development and deployment practices rely heavily on containerization. A candidate must be proficient with **Docker** and **Docker Compose**. The official Airflow documentation provides a docker-compose.yaml file that allows for the easy setup of a complete local Airflow environment, including the webserver, scheduler, and a PostgreSQL backend database.14 Mastering this setup is essential for creating reproducible and isolated environments, mirroring professional DevOps practices.

### **2.3. Principle 3: Integrating Heterogeneous Data Sources**

The power of Fair Supply's platform comes from its ability to fuse a client's private data with a vast array of external public and proprietary datasets.2 A data engineer in this environment must be adept at integrating these heterogeneous sources.

The learning path for this skill set includes:

* **API Integration and Creation:** Data is frequently exchanged via APIs. A candidate must not only know how to consume APIs but also how to build them.  
  * **Consuming APIs:** This involves writing robust client code that can handle authentication, pagination, and rate limiting when pulling data from third-party sources, such as commercial ESG score providers.18  
  * **Creating APIs:** A critical skill is the ability to expose processed data through a well-designed API. **FastAPI** is a modern, high-performance Python framework ideal for this purpose.21 Building a simple API with FastAPI that serves data from a project's database provides invaluable experience in defining data models (using Pydantic), creating endpoints for different HTTP methods (GET, POST, DELETE), and understanding the full data lifecycle.23  
* **Ethical Web Scraping:** When data is not available through a structured API (e.g., certain government reports or NGO publications), web scraping may be necessary. Learning libraries like **BeautifulSoup** or **Scrapy** to parse HTML and extract structured information is a valuable, if secondary, skill. This must always be done ethically and in accordance with a website's terms of service.  
* **Strategic Data Storage:** Different types of data require different storage solutions. A "one-size-fits-all" approach is inefficient. A sophisticated data engineer understands the trade-offs:  
  * **Data Lake (e.g., AWS S3, Google Cloud Storage):** Ideal for storing vast amounts of raw, unstructured, or semi-structured data in its native format. This is the landing zone for raw API responses, scraped web pages, and client-provided files.  
  * **Data Warehouse (e.g., Snowflake, BigQuery, PostgreSQL):** Designed for storing structured, cleaned, and analytics-ready data. This is where the final, transformed tables from the Airflow pipeline would reside, optimized for complex analytical queries.  
  * **Graph Database (e.g., Neo4j):** Specialized for storing and querying highly interconnected, relationship-centric data. As established, this is the ideal home for the supply chain network itself.

A successful candidate will demonstrate the ability to design and build systems that leverage the right tool for the right job, creating a cohesive data ecosystem that is scalable, auditable, and capable of integrating diverse information streams.

## **Part 3: Mapping the Unseen: Mastering Graph Neural Networks for Supply Chain Transparency**

Once a robust data engineering foundation is in place, the focus shifts to the advanced modeling techniques that unlock the unique insights Fair Supply provides. At the core of analyzing multi-tier supply chains is the ability to understand and reason about complex relationships. This is the domain of graph analytics, and more specifically, Graph Neural Networks (GNNs), which are essential for moving beyond simple data points to a holistic understanding of network-based risk.

### **3.1. Graph Foundations: From Theory to a Graph Database**

Before applying sophisticated machine learning models, one must first be able to represent, store, and query graph-structured data efficiently. Relational databases are poorly suited for this task, as traversing deep relationships requires numerous expensive JOIN operations. Graph databases are purpose-built for this challenge.

The learning path for graph foundations should focus on **Neo4j**, a leading native graph database:

* **The Property Graph Model:** The first step is to internalize the Neo4j data model. Unlike tables, data is organized into:  
  * **Nodes:** Entities such as a Company, Product, Location, or Person.  
  * **Relationships:** Directed connections between nodes that describe how they are related, such as SUPPLIES\_TO, LOCATED\_IN, or OFFICER\_OF.  
  * **Properties:** Key-value pairs that store data on both nodes and relationships (e.g., a Company node can have a name property; a SUPPLIES\_TO relationship can have a transaction\_value property).  
* **Mastering the Cypher Query Language:** Cypher is Neo4j's declarative query language, designed to be intuitive and expressive for graph patterns. Proficiency in Cypher is non-negotiable. The learning focus should be on:  
  * **Pattern Matching:** Using ASCII-art-like syntax to describe patterns of nodes and relationships to find in the graph (e.g., MATCH (c1:Company)--\>(c2:Company)). This is the fundamental operation for any supply chain query.25  
  * **Creating and Merging Data:** Understanding how to use CREATE to add new data and MERGE to create data uniquely (create if it doesn't exist, match if it does), which is essential for preventing duplicate entities when ingesting data.  
  * **Variable-Length Traversals:** Learning to query for paths of arbitrary depth (e.g., MATCH (c1:Company)--\>(c2:Company)) is how one would trace a supply chain up to 10 tiers deep, directly mirroring Fair Supply's core capability.  
* **Practical Data Modeling and Import:** Theory must be put into practice. An essential exercise is to set up a free Neo4j AuraDB instance and follow tutorials to model a domain and import data.25 A highly relevant task is to take a set of CSV files representing companies and their relationships and use the  
  LOAD CSV command in Cypher to populate the graph database. This exercise of modeling a supply chain—defining the node labels, relationship types, and properties—is a critical skill that precedes any GNN work.

### **3.2. Introduction to Graph Neural Networks (GNNs)**

GNNs are a class of deep learning models designed specifically to operate on graph-structured data. They have revolutionized the field by enabling models to learn from both the features of entities (nodes) and the structure of their relationships (edges). This is precisely what is needed to assess supply chain risk, where a company's risk profile is determined not just by its own actions but also by the actions of those it is connected to.

The core mechanism of GNNs is **message passing** or **neighborhood aggregation**.5 In each layer of a GNN, every node gathers feature information (messages) from its direct neighbors and aggregates them to update its own feature representation (embedding). By stacking multiple layers, a node's final embedding can capture information from its multi-hop neighborhood, effectively encoding its position and context within the wider graph.

A focused learning path should cover:

* **Key GNN Architectures:**  
  * **Graph Convolutional Networks (GCN):** The foundational GNN model that provides a simple and effective way to aggregate neighborhood information. It serves as an excellent starting point for understanding the core principles.27  
  * **Graph Attention Networks (GAT):** A more advanced and powerful architecture. Instead of treating all neighbors equally (like GCN), GATs use an attention mechanism to learn different weights for different neighbors. This is particularly crucial in heterogeneous supply chain networks, where a link to a major, high-volume supplier should likely have more influence on a node's risk profile than a link to a minor, one-off supplier.27  
* **PyTorch Geometric (PyG):** This is the industry-standard library for building and training GNNs in PyTorch. It provides an elegant and efficient framework that abstracts away much of the complexity of GNN implementation. Key components to master include:  
  * The Data object: PyG's fundamental data structure for representing a single graph, holding node features (x), graph connectivity (edge\_index), and other attributes.30  
  * GNN Layers: Pre-built, optimized implementations of common GNN layers like GCNConv and GATConv, which can be easily stacked to build deep models.31  
  * Mini-batching: PyG's specialized data loaders that can efficiently create mini-batches of multiple small graphs or sample subgraphs from a single giant graph, enabling training on large-scale datasets.

### **3.3. Applied GNNs for Supply Chain Analysis**

With a grasp of the theory and tools, the final step is to apply GNNs to the specific problems relevant to Fair Supply. The two most critical tasks are link prediction and node classification.

* **Link Prediction: Finding the Hidden Connections:**  
  * **The Task:** The goal of link prediction is to determine the probability of an edge existing between two nodes in a graph. In the context of a supply chain, this can be used to identify potential supplier relationships that are not explicitly declared or are hidden deep within the network—a direct application of "making the invisible visible".32  
  * **Learning Path:** A candidate should work through a PyG tutorial on link prediction.33 The typical approach involves training a model that takes the learned embeddings of a pair of nodes and passes them through a classifier to predict an edge. A critical aspect of this task is the data splitting methodology. Naively splitting edges into training, validation, and test sets can lead to data leakage, where the model can infer the existence of a test edge from the surrounding training graph structure. One must learn and implement proper graph splitting techniques to ensure a robust and reliable evaluation of the model's performance.35  
* **Node Classification: Assessing Supplier Risk:**  
  * **The Task:** The goal of node classification is to assign a label to each node in a graph. For supply chain analysis, this translates directly to classifying a company (a node) as "high-risk," "medium-risk," or "low-risk" for a specific ESG issue like modern slavery or deforestation.28  
  * **Learning Path:** A classic entry point is the Cora dataset tutorial, which is available in most GNN libraries.30 In this task, the GNN learns to classify academic papers (nodes) into different subjects based on their word-content (features) and their citation links (edges). This is a direct and powerful analogy for ESG risk assessment: the GNN learns to classify a company's risk level based on its own attributes (e.g., industry, location, public statements) and, crucially, the risk levels of the other companies it is connected to in the supply chain graph. This demonstrates the power of GNNs to propagate risk signals through the network.

By mastering these foundational graph database skills and applied GNN techniques, a candidate can demonstrate the exact advanced analytical capabilities required to power a platform like Fair Supply's.

## **Part 4: From Data to Dialogue: Advanced LLM Orchestration for Risk and Compliance**

The final technological pillar is Large Language Model (LLM) orchestration. In the context of a sophisticated ESG platform, LLMs are not used as simple chatbots for generic conversation. Instead, they are employed as powerful reasoning engines that can query, analyze, synthesize, and explain the complex data and graph structures built in the preceding stages. This allows users—who may be procurement professionals, not data scientists—to interact with the system using natural language to uncover deep insights.3

### **4.1. Beyond Basic RAG: Architecting an Advanced System**

A standard Retrieval-Augmented Generation (RAG) pipeline, which retrieves a few text chunks and feeds them to an LLM, is insufficient for the nuance and complexity of ESG risk analysis. Clients need to ask multi-faceted questions that require synthesizing information from dense regulatory reports, news articles, and the intricate structure of a supply chain graph. This necessitates an architecture built on advanced RAG techniques.37

A comprehensive learning path must include:

* **Mastering LangChain:** **LangChain** is the preeminent framework for composing and orchestrating LLM applications. It is essential to master its core abstractions, which allow for the creation of modular and powerful systems:  
  * **LLMs/ChatModels, PromptTemplates, and Output Parsers:** These are the basic building blocks for any interaction with an LLM.  
  * **LangChain Expression Language (LCEL):** This is a declarative way to chain these components together using a pipe (|) syntax, making complex workflows easy to define and modify.39  
* **Implementing Advanced Retrieval Strategies:** The quality of a RAG system's output is almost entirely dependent on the quality of its retrieval step. To build a system that can handle ESG queries, one must go beyond simple vector search:  
  * **Intelligent Chunking:** Fixed-size chunking is a blunt instrument. For processing long, structured documents like annual sustainability reports, techniques like MarkdownHeaderTextSplitter (which uses document headings to create logical chunks) or SemanticChunker (which groups semantically related sentences) are far more effective at preserving context.38  
  * **Hybrid Search:** Relying solely on semantic (vector) search can miss critical keywords, acronyms, or specific entity names common in ESG reports (e.g., "Xinjiang," "CSRD," "GHG Protocol"). A state-of-the-art retriever combines vector search with a traditional keyword search algorithm like **BM25**. This hybrid approach leverages the strengths of both—semantic understanding and lexical precision—to ensure comprehensive retrieval.38  
  * **Query Transformation:** User questions are often imprecise. A powerful technique is to use an LLM to transform the user's query before it hits the retrieval system. This can involve expanding acronyms, adding synonyms, or decomposing a complex question into several simpler sub-questions that can be executed independently.38  
  * **Reranking:** Retrieval systems often return a larger set of potentially relevant documents. A **reranking** step, which uses a more powerful but computationally expensive model (like a Cross-Encoder) to re-score and re-order the top results, can significantly improve the final relevance of the context passed to the LLM.38

### **4.2. Graph RAG: Conversing with Your Supply Chain**

The most powerful and relevant application for this learning plan is **Graph RAG**, which combines the LLM's reasoning capabilities with the structured knowledge stored in the graph database. This allows the system to answer questions that require both understanding unstructured text and traversing explicit relationships.

The development of this capability is predicated on a sophisticated understanding of the complementary roles of different database technologies. Vector databases are excellent for fast, semantic similarity search on unstructured text (e.g., finding relevant paragraphs in a report), but they inherently lose the relational context between data points.41 Graph databases, conversely, excel at storing and querying explicit, complex relationships (e.g., ownership structures, supplier links) but are not designed for semantic search on unstructured text properties within nodes.42

A truly advanced ESG intelligence system, therefore, uses both. It leverages the graph database to answer questions about the *structure* of the supply chain ("Who are the parent companies of this supplier?") and a vector database to answer questions about the *content* within documents ("Find me news articles that mention this company in the context of labor disputes"). The LLM acts as the central orchestrator, or **agent**, that intelligently decides which tool to use based on the user's query.

| Database Technology | Strengths | Weaknesses for RAG | Suitability for Supply Chain Analysis |
| :---- | :---- | :---- | :---- |
| **Vector Database** | Fast semantic search; handles various data types. | Loses relational context; crude chunking can lead to errors; scalability and cost issues with some algorithms. | Good for searching unstructured reports and news, but poor at understanding the supply chain structure itself. |
| **Graph Database** | Natively stores and queries complex relationships; models interconnected data effectively. | Inefficient for large-scale semantic search on text; less effective for queries across massive, sparse databases. | Excellent for modeling and querying the supply chain network, but needs a separate system for text search. |
| **Knowledge Graph (Combined Approach)** | Combines relational context (graph) with semantic search (vectors); preserves context; enables synthesis across sources. | Can be computationally expensive; requires expertise in semantic technologies and data modeling. | The ideal solution. It allows for answering complex, multi-hop questions that require both structural and semantic understanding. |

Data sourced from 41

The learning path to build this system involves:

* **Building a LangChain Agent:** A candidate must learn to build a LangChain agent, which is an LLM-powered decision-maker. Following a tutorial on building a Graph RAG chatbot is the most direct path.40  
* **Creating Custom Tools:** The agent is given access to a set of "tools" that it can choose to use. For this use case, two primary tools are required:  
  1. **A Cypher Chain Tool:** This tool is capable of taking a natural language question, converting it into a Cypher query, executing it against the Neo4j database, and returning the result.  
  2. **A Vector Store Retriever Tool:** This is a standard RAG tool that performs semantic search over a vector index of documents.  
* **Orchestration in Action:** The agent, powered by a capable model like GPT-4, learns to decompose a complex user query. For example, given the query: "Summarize the biodiversity risks associated with Company Y's suppliers in Brazil," the agent would reason as follows:  
  1. "First, I need to identify Company Y's suppliers located in Brazil. The best tool for this is the Cypher Chain, as it can query the graph structure."  
  2. It uses the Cypher tool to run MATCH (c:Company {name: 'Company Y'})--\>(s:Supplier)--\>(l:Location {country: 'Brazil'}) RETURN s.name.  
  3. "Now that I have a list of suppliers, I need to find information about their biodiversity risks. The best tool for this is the vector store retriever, which can search our document corpus."  
  4. It uses the vector retriever tool to search for documents mentioning the identified suppliers and terms like "biodiversity," "deforestation," or "ecosystem impact."  
  5. Finally, it synthesizes the results from both tools into a coherent, natural language answer for the user.

### **4.3. LLM Evaluation: Ensuring Trust and Accuracy**

For a company like Fair Supply, whose brand is built on "precision" and "defensible results," the output of an LLM cannot be a black box.1 A system that hallucinates or provides factually incorrect answers is not just unhelpful; it is a liability. Therefore, mastering the evaluation of LLM and RAG systems is a non-negotiable, production-critical skill.

The learning path must focus on implementing a rigorous evaluation framework:

* **Mastering Key RAG Metrics:** One must learn to programmatically measure the quality of the RAG system's outputs. The most important metrics include:  
  * **Faithfulness:** Does every claim in the generated answer derive directly and verifiably from the provided context documents? This is the primary defense against hallucination.  
  * **Answer Relevancy:** Is the answer actually relevant to the user's question?  
  * **Context Precision:** Of the documents retrieved, how many were actually relevant to answering the question?  
  * **Context Recall:** Of all the relevant documents that *could* have been retrieved, how many actually were?  
* **Implementing an Evaluation Pipeline:** The most effective way to measure these metrics is to build an automated evaluation pipeline. This process, as described in frameworks from providers like Datadog, involves several steps 43:  
  1. **Create a "Golden Dataset":** Manually curate a high-quality test set of questions, the ideal context documents needed to answer them, and the ground-truth correct answers. This requires significant human expertise.  
  2. **Use "LLM-as-a-Judge":** For metrics like Faithfulness and Relevancy, which are semantic in nature, an effective technique is to use another powerful LLM as an impartial "judge." The judge LLM is given the question, the generated answer, and the retrieved context, and is prompted to score the answer on a specific metric (e.g., "On a scale of 1-5, how faithful is this answer to the provided context?").  
  3. **Automate and Monitor:** This evaluation pipeline should be run automatically whenever changes are made to the RAG system (e.g., a new prompt, a new LLM, a new retrieval strategy). The results should be tracked over time to monitor for performance regressions. Open-source tools like **Ragas** or **DeepEval** provide excellent frameworks for implementing such pipelines.

By mastering these advanced orchestration and evaluation techniques, a candidate can demonstrate the ability to build not just a functional LLM application, but a reliable, trustworthy, and production-ready AI system.

## **Part 5: The Capstone Initiative: Integrated Projects to Demonstrate Mastery**

Theoretical knowledge and isolated skills are valuable, but the ability to synthesize them into a cohesive, end-to-end system is what distinguishes a top-tier candidate. The following capstone projects are designed to be serious, portfolio-worthy undertakings that directly simulate the work done at Fair Supply. They are not simple tutorials; they are blueprints for building systems that integrate data engineering, graph neural networks, and advanced LLM orchestration to solve real-world ESG challenges. Completing one or both of these projects will provide an undeniable demonstration of the specific, high-impact skills Fair Supply seeks.

### **5.1. Project 1: The Modern Slavery Risk Detection Engine**

**Objective:** To construct a comprehensive system that can ingest corporate ownership and supplier data, map it into a knowledge graph, enrich it with predicted links, and allow a user to query this graph using natural language to uncover hidden modern slavery risks.

**Architecture and Implementation Steps:**

1. **Data Engineering and Pipeline (Apache Airflow):**  
   * The foundation is an automated data ingestion pipeline. An Airflow DAG will be created to perform the following tasks on a schedule (e.g., weekly):  
   * **Task 1 \- Ingest Corporate Ownership Data:** Download the latest corporate ownership and control datasets from **OpenSanctions KYB**.44 This invaluable open-source project aggregates data from numerous national corporate registries, providing information on companies, their subsidiaries, and their "Persons with Significant Control".45 The data is available as bulk downloads and structured in the FollowTheMoney format.44  
   * **Task 2 \- Ingest Risk Data:** Download and process data from **Walk Free's Global Slavery Index**.46 Specifically, extract the list of goods (e.g., electronics, garments, palm oil) and source countries that are identified as having a high risk of being produced with forced labor.46  
   * **Task 3 \- Load to Staging:** The pipeline will load this raw data into a staging area in a PostgreSQL database for cleaning and pre-processing.  
2. **Graph Database Modeling and Population (Neo4j):**  
   * The processed data will be used to construct a detailed knowledge graph in Neo4j. The graph schema will include nodes for Company, Person, Location, and ProductRisk.  
   * Relationships will be created to represent OWNS, CONTROLS, LOCATED\_IN, and PRODUCES\_RISKY\_GOOD.  
   * A hypothetical list of a target company's Tier 1 suppliers (created for the project) will be ingested, linking them into this broader graph.  
3. **Graph Neural Network for Link Prediction (PyTorch Geometric):**  
   * This step demonstrates the ability to "find the invisible." A **Graph Attention Network (GAT)** will be trained on the large OpenSanctions corporate ownership graph.  
   * The model's task will be **link prediction**: given two entities (e.g., two Person nodes, or a Person and a Company), predict the probability that a controlling relationship exists between them.  
   * The trained model can then be applied to the project's graph to infer potential, non-obvious connections, such as a director of a supplier company also being a shareholder in a separate entity operating in a high-risk sector. These predicted links will be added to the graph as a new relationship type, e.g., PREDICTED\_LINK, with a confidence score.  
4. **LLM Orchestration and Querying (LangChain):**  
   * **Vector Store Creation:** A corpus of documents will be created, including corporate annual reports (from the SEC EDGAR database), NGO reports on modern slavery, and relevant news articles. These documents will be chunked and indexed into a vector store (e.g., FAISS).  
   * **Graph RAG Agent:** A sophisticated LangChain agent will be built, as described in Part 4.2. It will have access to two primary tools: a **Cypher Chain** to query the Neo4j graph and a **Vector Retriever** to search the document index.  
   * **Demonstration Query:** The final system will be able to answer complex, multi-hop questions posed in natural language. For example: *"For, identify any Tier 1 suppliers that have direct or predicted ownership links to individuals or companies known to operate in the 'Garments' sector in a country flagged by the Global Slavery Index. Then, summarize any negative news or report findings related to these identified suppliers."* The agent would use the graph to find the structural connections and the vector store to find the supporting textual evidence, synthesizing them into a single, actionable answer.

### **5.2. Project 2: The Deforestation-Free Commodity Tracer**

**Objective:** To build a data pipeline and analysis tool that traces a forest-risk commodity (e.g., soy, palm oil, beef) from an importing country back to its likely sub-national regions of origin, using GNNs to assess and propagate a deforestation risk score through the trade network.

**Architecture and Implementation Steps:**

1. **Data Engineering and Pipeline (Apache Airflow):**  
   * An Airflow DAG will be designed to collect the necessary trade and environmental data:  
   * **Task 1 \- Ingest Trade Data:** Download international trade data from the **UN Comtrade Database**.47 This dataset provides detailed monthly statistics on the import and export of goods between countries, including values and volumes. The pipeline will filter for a specific commodity (e.g., HS code for soybeans).  
   * **Task 2 \- Ingest Deforestation Data:** Using the API for **Global Forest Watch (GFW)**, the pipeline will download geospatial data on tree cover loss and fire alerts for the primary exporting countries identified in the trade data.48 This data is often available at a granular, sub-national level.  
   * **Task 3 \- Ingest Trader Data (Optional Extension):** For a more advanced version, data from **Trase.earth** can be integrated. Trase provides data that links specific commodity traders to sub-national sourcing regions and their associated deforestation risk.49  
2. **Graph Database and GNNs (Neo4j & PyTorch Geometric):**  
   * **Graph Modeling:** The global trade flow for the commodity will be modeled as a graph. Nodes could represent Country, SourcingRegion (e.g., a Brazilian municipality), Port, and Trader. Edges (TRADE\_FLOW) would represent the movement of the commodity, with properties for volume and value.  
   * **GNN for Risk Classification:** This step applies GNNs for a **graph classification** task. A GNN model will be trained to classify each SourcingRegion node as "high-risk," "medium-risk," or "low-risk." The features for each node will be derived from the GFW data (e.g., recent tree cover loss rate, number of fire alerts). The GNN will learn not only from these features but also from the connectivity patterns (e.g., a region might be higher risk if it's geographically adjacent to other high-risk regions). This risk score can then be propagated through the network along the trade flow edges.  
3. **LLM Orchestration and Visualization (LangChain & Streamlit):**  
   * **Interactive Front-End:** A simple, interactive web application will be built using **Streamlit** or a **FastAPI**\-backed front-end. This makes the complex analysis accessible to a non-technical user.  
   * **User Interaction:** The user will be able to select a commodity and an importing country/region (e.g., "Soybeans" and "European Union").  
   * **LLM-Powered Reporting:** Upon user selection, a LangChain agent will execute a series of actions:  
     1. Query the Neo4j graph to identify the main trade pathways, sourcing countries, and sub-national regions for that import flow.  
     2. Retrieve the pre-calculated deforestation risk scores for those sourcing regions.  
     3. Synthesize this structured data into a clear, narrative summary. For example: *"An analysis of the EU's soybean imports from Brazil shows that approximately 40% originates from municipalities within the state of Mato Grosso. These regions have a GNN-derived aggregate deforestation risk score of 8.5/10, driven by a 15% increase in tree cover loss alerts over the past quarter, indicating a significant risk exposure for importers sourcing from this area."*

These projects, when completed and documented thoroughly, will serve as powerful evidence of a candidate's ability to tackle the complex, data-intensive, and purpose-driven challenges central to Fair Supply's mission.

## **Part 6: Strategic Positioning: Aligning Your Profile with Fair Supply's Ethos**

Successfully completing the learning plan and capstone projects is a significant achievement. However, the final step is to strategically package and present this work to resonate with Fair Supply's specific culture and values. Technical skill alone is not enough; the ideal candidate must also demonstrate clear communication, a collaborative spirit, and a genuine alignment with the company's purpose.

### **6.1. Crafting Your Technical Narrative**

The way skills and projects are presented on a resume and in a portfolio can dramatically alter their perceived value. The narrative should be framed around solving problems, not just using tools.

* **The High-Impact Resume:** Avoid a simple laundry list of technologies. Instead, for each capstone project, use the **"Problem-Action-Result" (PAR)** framework to craft compelling bullet points. This shifts the focus from what was used to what was achieved.  
  * *Instead of:* "Used GNNs and Neo4j."  
  * *Use:* "**Problem:** Lack of transparency in multi-tier corporate ownership structures obscures ESG risk. **Action:** Built a GNN-based link prediction model using PyTorch Geometric on the OpenSanctions graph database to uncover hidden beneficial ownership connections. **Result:** The model successfully identified 15% more high-risk associations between suppliers and sanctioned entities than could be found through direct, declared links alone."  
  * This approach demonstrates not only technical competence but also a commercial and problem-oriented mindset.  
* **The Professional GitHub Portfolio:** For a technical role at a company like Fair Supply, a candidate's GitHub profile is a primary exhibit. It is not a code dump; it is a professional showcase. Each project repository must be treated as a product:  
  * **A Comprehensive README.md:** This is the most critical file. It should be a well-structured document that clearly explains the project's **"why"** (the business problem it solves, e.g., "Identifying Modern Slavery Risk in Opaque Supply Chains").  
  * **Architecture and Methodology:** The README should include a clear architecture diagram showing how the components (Airflow, Neo4j, LangChain Agent, etc.) interact. It must also detail the methodology—why a GAT was chosen over a GCN, what chunking strategy was used for the RAG system, how the model was evaluated.  
  * **Instructions and Demo:** It must provide clear, step-by-step instructions on how to set up the environment (e.g., using docker-compose) and run the code. Including a short GIF or video demonstrating the final application in action is highly effective. This level of documentation demonstrates professionalism, strong communication skills, and consideration for others who might interact with the work.

### **6.2. Preparing for the Interview**

The interview is the opportunity to bring the resume and portfolio to life, demonstrating not just technical depth but also cultural fit. Fair Supply's culture is explicitly defined by principles like "Everyone leads all the time," "Build from the bottom up," and "No drama. A zero tolerance for pride and ego".1

* **Anticipate Deep Technical Questions:** Be prepared to defend every technical decision made in the capstone projects. The founders are an industrial mathematician and a human rights lawyer; they value precision and rigor.1 Expect questions like:  
  * "Why did you choose a Graph Attention Network for your link prediction task? What are its advantages and disadvantages compared to a simpler Graph Convolutional Network in the context of corporate ownership data?"  
  * "How did you evaluate your RAG system's faithfulness? What steps did you take to minimize hallucinations, and what are the remaining limitations of your approach?"  
  * "You used the MRIO methodology as an inspiration. What are the primary data challenges in constructing and maintaining a real-world MRIO model for ESG risk analysis?"  
* **Align with Cultural Values:** Frame answers in a way that reflects their stated culture.  
  * When discussing a project, talk about the ownership taken ("I decided to build the evaluation pipeline first because I knew the system's defensibility was paramount").  
  * When asked about a challenge, focus on transparent communication and a collaborative, no-ego approach to problem-solving.  
  * Show that you see yourself as more than a coder; you are a "leader, stakeholder and steward" of the work and the mission.1  
* **Demonstrate Genuine Purpose:** This is perhaps the most crucial element for a purpose-driven company. The passion for the mission must be authentic. Connect the technical work back to the real-world impact. The candidate didn't just build a GNN; they built a tool that could help a company ensure its supply chain is free from forced labor. They didn't just create a data pipeline; they created a system to help protect biodiversity. This demonstrated passion, combined with deep and relevant technical expertise, is the hallmark of a candidate who is not just looking for a job, but is seeking to "do the best work of your life" in service of a meaningful goal.50 This alignment is the ultimate competitive advantage.

#### **Works cited**

1. About Us – Leading ESG Risk Management & Compliance \- Fair Supply, accessed on June 25, 2025, [https://www.fairsupply.com/about](https://www.fairsupply.com/about)  
2. Investing in FairSupply – Airtree Ventures, accessed on June 25, 2025, [https://www.airtree.vc/open-source-vc/our-investment-in-fairsupply](https://www.airtree.vc/open-source-vc/our-investment-in-fairsupply)  
3. ESG Risk & Reporting Platform \- Fair Supply, accessed on June 25, 2025, [https://www.fairsupply.com/platform/platform-overview](https://www.fairsupply.com/platform/platform-overview)  
4. ESG Data for Corporate Sustainability & Compliance \- Fair Supply, accessed on June 25, 2025, [https://www.fairsupply.com/use-cases/sustainability](https://www.fairsupply.com/use-cases/sustainability)  
5. Graph Neural Network for Daily Supply Chain Problems\[v1 ..., accessed on June 25, 2025, [https://www.preprints.org/manuscript/202409.2376/v1](https://www.preprints.org/manuscript/202409.2376/v1)  
6. Graph Neural Network for Daily Supply Chain Problems \- ResearchGate, accessed on June 25, 2025, [https://www.researchgate.net/publication/384592034\_Graph\_Neural\_Network\_for\_Daily\_Supply\_Chain\_Problems](https://www.researchgate.net/publication/384592034_Graph_Neural_Network_for_Daily_Supply_Chain_Problems)  
7. Fair Supply \- Cloud-based ESG Risk Management and Compliance, accessed on June 25, 2025, [https://fairsupply.webflow.io/](https://fairsupply.webflow.io/)  
8. Our Team \- How the magic happens \- Fair Supply, accessed on June 25, 2025, [https://www.fairsupply.com/team](https://www.fairsupply.com/team)  
9. Sustainability Spotlight — Comparison of Significant Sustainability ..., accessed on June 25, 2025, [https://dart.deloitte.com/USDART/home/publications/deloitte/sustainability-spotlight/2025/sustainability-related-reporting-requirements-and-standards](https://dart.deloitte.com/USDART/home/publications/deloitte/sustainability-spotlight/2025/sustainability-related-reporting-requirements-and-standards)  
10. Comparison of Significant Sustainability-Related Reporting Requirements \- The Harvard Law School Forum on Corporate Governance, accessed on June 25, 2025, [https://corpgov.law.harvard.edu/2025/06/19/comparison-of-significant-sustainability-related-reporting-requirements/](https://corpgov.law.harvard.edu/2025/06/19/comparison-of-significant-sustainability-related-reporting-requirements/)  
11. Your guide to global sustainability reporting frameworks \- Sweep, accessed on June 25, 2025, [https://www.sweep.net/blog/your-guide-to-global-sustainability-reporting-frameworks](https://www.sweep.net/blog/your-guide-to-global-sustainability-reporting-frameworks)  
12. Fair Supply \- 2025 Company Profile, Team, Funding & Competitors ..., accessed on June 25, 2025, [https://tracxn.com/d/companies/fair-supply/\_\_raph0TClqV8lG3vMkKtrwOUbH8vnl6R4FT1i3NCfeLw](https://tracxn.com/d/companies/fair-supply/__raph0TClqV8lG3vMkKtrwOUbH8vnl6R4FT1i3NCfeLw)  
13. The Role of Data Engineers in ESG Reporting | Orchestra, accessed on June 25, 2025, [https://www.getorchestra.io/guides/the-role-of-data-engineers-in-esg-reporting](https://www.getorchestra.io/guides/the-role-of-data-engineers-in-esg-reporting)  
14. Building a Simple Data Pipeline \- Apache Airflow, accessed on June 25, 2025, [https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/pipeline.html)  
15. Building a Running Pipeline \- Apache Airflow, accessed on June 25, 2025, [https://airflow.apache.org/docs/apache-airflow/2.6.1/tutorial/pipeline.html](https://airflow.apache.org/docs/apache-airflow/2.6.1/tutorial/pipeline.html)  
16. Scheduling Data Pipelines with Apache Airflow: A Beginner's Guide, accessed on June 25, 2025, [https://www.dasca.org/world-of-data-science/article/scheduling-data-pipelines-with-apache-airflow-a-beginners-guide](https://www.dasca.org/world-of-data-science/article/scheduling-data-pipelines-with-apache-airflow-a-beginners-guide)  
17. Apache Airflow One Shot- Building End To End ETL Pipeline Using AirFlow And Astro, accessed on June 25, 2025, [https://www.youtube.com/watch?v=Y\_vQyMljDsE](https://www.youtube.com/watch?v=Y_vQyMljDsE)  
18. ESG Scores API \- Tradefeeds, accessed on June 25, 2025, [https://tradefeeds.com/esg-scores-api/](https://tradefeeds.com/esg-scores-api/)  
19. ESG Data API \- MSCI \- Developer Community, accessed on June 25, 2025, [https://developer.msci.com/apis/esg-data-api-v3-0](https://developer.msci.com/apis/esg-data-api-v3-0)  
20. ESG Data API \- ESG Data Requirements \- Bavest Blog, accessed on June 25, 2025, [https://www.bavest.co/en/post/esg-api-data-api-what-you-need-to-know](https://www.bavest.co/en/post/esg-api-data-api-what-you-need-to-know)  
21. Creating REST API using FastAPI \- Srikanth Technologies, accessed on June 25, 2025, [http://srikanthtechnologies.com/blog/python/fastapi.aspx](http://srikanthtechnologies.com/blog/python/fastapi.aspx)  
22. Creating First REST API with FastAPI \- GeeksforGeeks, accessed on June 25, 2025, [https://www.geeksforgeeks.org/python/creating-first-rest-api-with-fastapi/](https://www.geeksforgeeks.org/python/creating-first-rest-api-with-fastapi/)  
23. First Steps \- FastAPI, accessed on June 25, 2025, [https://fastapi.tiangolo.com/tutorial/first-steps/](https://fastapi.tiangolo.com/tutorial/first-steps/)  
24. Building RESTful APIs with FastAPI \- GeeksforGeeks, accessed on June 25, 2025, [https://www.geeksforgeeks.org/building-restful-apis-with-fastapi/](https://www.geeksforgeeks.org/building-restful-apis-with-fastapi/)  
25. Neo4j Tutorial: Using And Querying Graph Databases in Python \- DataCamp, accessed on June 25, 2025, [https://www.datacamp.com/tutorial/neo4j-tutorial](https://www.datacamp.com/tutorial/neo4j-tutorial)  
26. Tutorials \- Getting Started \- Neo4j, accessed on June 25, 2025, [https://neo4j.com/docs/getting-started/appendix/tutorials/tutorials-overview/](https://neo4j.com/docs/getting-started/appendix/tutorials/tutorials-overview/)  
27. A Benchmark Dataset for Supply Chain Planning using Graph Neural Networks \- arXiv, accessed on June 25, 2025, [https://arxiv.org/html/2401.15299v1](https://arxiv.org/html/2401.15299v1)  
28. Optimizing Supply Chain Networks with the Power of Graph Neural Networks \- arXiv, accessed on June 25, 2025, [https://arxiv.org/html/2501.06221v1](https://arxiv.org/html/2501.06221v1)  
29. Graph Neural Network-Based Predictive Modeling for Enhanced Supply Chain Resilience against Multi-Modal Disruptions | Journal of Information Systems Engineering and Management, accessed on June 25, 2025, [https://jisem-journal.com/index.php/journal/article/view/5571](https://jisem-journal.com/index.php/journal/article/view/5571)  
30. Tutorial 6: Basics of Graph Neural Networks \- Lightning AI, accessed on June 25, 2025, [https://lightning.ai/docs/pytorch/stable/notebooks/course\_UvA-DL/06-graph-neural-networks.html](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/06-graph-neural-networks.html)  
31. pyg-team/pytorch\_geometric: Graph Neural Network Library for PyTorch \- GitHub, accessed on June 25, 2025, [https://github.com/pyg-team/pytorch\_geometric](https://github.com/pyg-team/pytorch_geometric)  
32. www.repository.cam.ac.uk, accessed on June 25, 2025, [https://www.repository.cam.ac.uk/bitstreams/92a7b226-2f41-4703-ad15-69c53ec7f26a/download\#:\~:text=Using%20GNN%2C%20we%20could%20predict,risks%20they%20are%20exposed%20to.](https://www.repository.cam.ac.uk/bitstreams/92a7b226-2f41-4703-ad15-69c53ec7f26a/download#:~:text=Using%20GNN%2C%20we%20could%20predict,risks%20they%20are%20exposed%20to.)  
33. Colab Notebooks and Video Tutorials \- PyTorch Geometric \- Read the Docs, accessed on June 25, 2025, [https://pytorch-geometric.readthedocs.io/en/2.6.1/get\_started/colabs.html](https://pytorch-geometric.readthedocs.io/en/2.6.1/get_started/colabs.html)  
34. Colab Notebooks and Video Tutorials — pytorch\_geometric 2.0.3 documentation, accessed on June 25, 2025, [https://pytorch-geometric.readthedocs.io/en/2.0.3/notes/colabs.html](https://pytorch-geometric.readthedocs.io/en/2.0.3/notes/colabs.html)  
35. GRAPH Link Prediction w/ DGL on Pytorch and PyG Code Example | GraphML \- YouTube, accessed on June 25, 2025, [https://www.youtube.com/watch?v=wxJ84sMJfUA](https://www.youtube.com/watch?v=wxJ84sMJfUA)  
36. Buzz Usborne — FairSupply, accessed on June 25, 2025, [https://buzzusborne.com/work/fairsupply/](https://buzzusborne.com/work/fairsupply/)  
37. Advanced RAG Techniques: Boost Accuracy & Efficiency \- Chitika, accessed on June 25, 2025, [https://www.chitika.com/advanced-rag-techniques-guide/](https://www.chitika.com/advanced-rag-techniques-guide/)  
38. RAG techniques: From naive to advanced \- Weights & Biases \- Wandb, accessed on June 25, 2025, [https://wandb.ai/site/articles/rag-techniques/](https://wandb.ai/site/articles/rag-techniques/)  
39. Building a Secure RAG with Python, LangChain, and OpenFGA | Auth0, accessed on June 25, 2025, [https://auth0.com/blog/building-a-secure-rag-with-python-langchain-and-openfga/](https://auth0.com/blog/building-a-secure-rag-with-python-langchain-and-openfga/)  
40. Build an LLM RAG Chatbot With LangChain \- Real Python, accessed on June 25, 2025, [https://realpython.com/build-llm-rag-chatbot-with-langchain/](https://realpython.com/build-llm-rag-chatbot-with-langchain/)  
41. writer.com, accessed on June 25, 2025, [https://writer.com/engineering/vector-database-vs-graph-database/\#:\~:text=Whereas%20vector%20databases%20often%20lose,databases%20worth%20considering%20for%20RAG.](https://writer.com/engineering/vector-database-vs-graph-database/#:~:text=Whereas%20vector%20databases%20often%20lose,databases%20worth%20considering%20for%20RAG.)  
42. Vector database vs. graph database: Knowledge Graph impact ..., accessed on June 25, 2025, [https://writer.com/engineering/vector-database-vs-graph-database/](https://writer.com/engineering/vector-database-vs-graph-database/)  
43. Building an LLM evaluation framework: best practices | Datadog, accessed on June 25, 2025, [https://www.datadoghq.com/blog/llm-evaluation-framework-best-practices/](https://www.datadoghq.com/blog/llm-evaluation-framework-best-practices/)  
44. Know-your-business datasets \- OpenSanctions, accessed on June 25, 2025, [https://www.opensanctions.org/kyb/](https://www.opensanctions.org/kyb/)  
45. Our bulk data distributions \- OpenSanctions, accessed on June 25, 2025, [https://www.opensanctions.org/datasets/](https://www.opensanctions.org/datasets/)  
46. Global Slavery Index | Walk Free, accessed on June 25, 2025, [https://www.walkfree.org/global-slavery-index/](https://www.walkfree.org/global-slavery-index/)  
47. Data Sources \- Supply Chain Management \- Research Guides at University of Tennessee Knoxville, accessed on June 25, 2025, [https://libguides.utk.edu/c.php?g=911741\&p=6575936](https://libguides.utk.edu/c.php?g=911741&p=6575936)  
48. Causes of Forest Loss | World Resources Institute, accessed on June 25, 2025, [https://www.wri.org/insights/forest-loss-drivers-data-trends](https://www.wri.org/insights/forest-loss-drivers-data-trends)  
49. Introducing the Deforestation Risk Toolset \- Accountability Framework Initiative, accessed on June 25, 2025, [https://accountability-framework.org/pt/news-events/news/article/introducing-the-deforestation-risk-toolset/](https://accountability-framework.org/pt/news-events/news/article/introducing-the-deforestation-risk-toolset/)  
50. Careers \- Fair Supply, accessed on June 25, 2025, [https://www.fairsupply.com/careers](https://www.fairsupply.com/careers)  
51. Careers \- Fair Supply, accessed on June 25, 2025, [https://fairsupply.webflow.io/careers](https://fairsupply.webflow.io/careers)